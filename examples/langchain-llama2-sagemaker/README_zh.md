---
title: éƒ¨ç½²é›†æˆäº† Llama2 çš„ LangChain åº”ç”¨
description: ä½¿ç”¨ Pluto ä½¿ LangChian åº”ç”¨ç¨‹åºè½»æ¾æ¥å…¥ Llama2 å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶æœ€ç»ˆå°† LangChain åº”ç”¨äº§å“åŒ–éƒ¨ç½²åˆ° AWS äº‘å¹³å°ä¸Šï¼Œæš´éœ²å‡º HTTP æ¥å£ã€‚
tags: ["AWS", "TypeScript", "LangChain", "Llama2"]
---

# éƒ¨ç½²é›†æˆäº† Llama2 çš„ LangChain åº”ç”¨

è¿™ç¯‡æ–‡æ¡£å°†ä»‹ç»ä½¿ç”¨ Pluto ä½¿ LangChian åº”ç”¨ç¨‹åºè½»æ¾æ¥å…¥ Llama2 å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶æœ€ç»ˆå°† LangChain åº”ç”¨äº§å“åŒ–éƒ¨ç½²åˆ° AWS äº‘å¹³å°ä¸Šï¼Œæš´éœ²å‡º HTTP æ¥å£ã€‚

è¿™ç¯‡æ–‡æ¡£æœ€ç»ˆä¼šåœ¨ AWS å¹³å°ä¸Šåˆ›å»ºä¸€ä¸ª SageMaker å®ä¾‹æ¥éƒ¨ç½²ä¸€ä¸ª [TinyLlama 1.1B](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) å¤§è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶ä¼šåˆ›å»ºä¸¤ä¸ª Lambda å®ä¾‹ï¼Œåˆ†åˆ«åŸºäº LangChain å’Œéƒ¨ç½²çš„å¤§è¯­è¨€æ¨¡å‹å®ç°æœ€åŸºæœ¬çš„**å¯¹è¯**å’ŒåŸºäºæ–‡æ¡£çš„**é—®ç­”**ä¸¤ä¸ªåŠŸèƒ½ã€‚

æ•´ä¸ªç ”å‘è¿‡ç¨‹ï¼Œå¼€å‘è€…ä¸éœ€è¦å…³å¿ƒæ¨¡å‹éƒ¨ç½²ã€AWS èµ„æºé…ç½®ç­‰çäº‹ï¼Œåªéœ€è¦å…³æ³¨ä¸šåŠ¡é€»è¾‘çš„å®ç°å³å¯ã€‚å½“ç„¶ï¼Œè¿™ç¯‡æ–‡æ¡£åŒæ ·é€‚ç”¨äºéœ€è¦éƒ¨ç½²ä¸æ¥å…¥å…¶ä»–å¼€æºæ¨¡å‹çš„åœºæ™¯ã€‚

<details><summary>å±•å¼€æŸ¥çœ‹ç¤ºä¾‹åº”ç”¨çš„å…¨éƒ¨ä»£ç </summary>

````typescript
import { SageMaker, Function } from "@plutolang/pluto";
import { loadQAChain } from "langchain/chains";
import { Document } from "langchain/document";
import { PromptTemplate } from "langchain/prompts";
import {
  SageMakerEndpoint,
  SageMakerLLMContentHandler,
} from "@langchain/community/llms/sagemaker_endpoint";

/**
 * Deploy the Llama2 model on AWS SageMaker using the Hugging Face Text Generation Inference (TGI)
 * container. Here will deploy the TinyLlama-1.1B-Chat-v1.0 model, which can be run on the
 * ml.m5.xlarge instance.
 *
 * Below is a set up minimum requirements for each model size of Llama2 model:
 * ```
 * Model      Instance Type    Quantization    # of GPUs per replica
 * Llama 7B   ml.g5.2xlarge    -               1
 * Llama 13B  ml.g5.12xlarge   -               4
 * Llama 70B  ml.g5.48xlarge   bitsandbytes    8
 * Llama 70B  ml.p4d.24xlarge  -               8
 * ```
 * The initial limit set for these instances is zero. If you need more, you can request an increase
 * in quota via the [AWS Management Console](https://console.aws.amazon.com/servicequotas/home).
 */
const sagemaker = new SageMaker(
  "llama-2-7b",
  "763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.0-gpu-py310-cu121-ubuntu20.04",
  {
    instanceType: "ml.m5.xlarge",
    envs: {
      // HF_MODEL_ID: "meta-llama/Llama-2-7b-chat-hf",
      HF_MODEL_ID: "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
      HF_TASK: "text-generation",
      // If you want to deploy the Meta Llama2 model, you need to request a permission and prepare the
      // token. You can get the token from https://huggingface.co/settings/tokens
      // HUGGING_FACE_HUB_TOKEN: "hf_EmXPwpnyxxxxxxx"
    },
  }
);

/**
 * TODO: Given the constraints of the current version of Deducer, we have to place the following
 * code within the separated function. Once we've upgraded Deducer, it'll be necessary to move this
 * code outside of the function.
 */
async function createSageMakerModel() {
  // Custom for whatever model you'll be using
  class LLama27BHandler implements SageMakerLLMContentHandler {
    contentType = "application/json";

    accepts = "application/json";

    async transformInput(prompt: string, modelKwargs: Record<string, unknown>): Promise<any> {
      const payload = {
        inputs: prompt,
        parameters: modelKwargs,
      };
      const stringifiedPayload = JSON.stringify(payload);
      return new TextEncoder().encode(stringifiedPayload);
    }

    async transformOutput(output: any): Promise<string> {
      const response_json = JSON.parse(new TextDecoder("utf-8").decode(output));
      const content: string = response_json[0]["generated_text"] ?? "";
      return content;
    }
  }

  return new SageMakerEndpoint({
    endpointName: sagemaker.endpointName,
    modelKwargs: {
      temperature: 0.5,
      max_new_tokens: 700,
      top_p: 0.9,
    },
    endpointKwargs: {
      CustomAttributes: "accept_eula=true",
    },
    contentHandler: new LLama27BHandler(),
    clientOptions: {
      // In theory, there's no need to supply the following details as the code will be executed within
      // the AWS Lambda environment. However, due to the way SageMakerEndpoint is implemented, it's
      // required to specify a region.
      region: process.env["AWS_REGION"],
      // credentials: {
      //   accessKeyId: "YOUR AWS ACCESS ID",
      //   secretAccessKey: "YOUR AWS SECRET ACCESS KEY",
      // },
    },
  });

  // TODO: Use the following statement to help the deducer identify the right relationship between
  // Lambda and SageMaker. This will be used to grant permission for the Lambda instance to call
  // upon the SageMaker endpoint. This code should be removed after the deducer supports the
  // analysis of libraries.
  // TODO: bug: only asynchrous function can be successfully analyzed by deducer.
  await sagemaker.invoke({});
}

/**
 * Why we don't use the Router (Api Gateway) to handle the requests? Because the ApiGateway comes
 * with a built-in 30-second timeout limit, which unfortunately, can't be increased. This means if
 * the generation process takes longer than this half-minute window, we'll end up getting hit with a
 * 503 Service Unavailable error. Consequently, we directly use the Lambda function to handle the
 * requests.
 *
 * For more details, check out:
 * https://docs.aws.amazon.com/apigateway/latest/developerguide/limits.html
 *
 * You can send a POST HTTP request to the Lambda function using curl or Postman. The request body
 * needs to be set as an array representing the function's arguments. Here's an example of a curl
 * request:
 * ```sh
 * curl -X POST https://<your-lambda-url-id>.lambda-url.<region>.on.aws/ \
 *   -H "Content-Type: application/json" \
 *   -d '["What is the capital of France?"]'
 * ```
 * If you get an error message such as `{"code":400,"body":"Payload should be an array."}`, you can
 * add a query parameter, such as `?n=1`, to the URL to resolve it. I don't know why it turns into a
 * GET request when I don't include the query parameter, even though the curl log indicates it's a
 * POST request. If you know the reason, please let me know.
 */

/**
 * The following code is creating a chain for the chatbot task. It can answer the user-provided question.
 */
// TODO: Bug: The deducer fails to identify the function's resources if the return value of the
// constructor isn't assigned to a variable.
const chatFunc = new Function(
  async (query) => {
    const model = await createSageMakerModel();
    const promptTemplate = PromptTemplate.fromTemplate(`<|system|>
You are a cool and aloof robot, answering questions very briefly and directly.</s>
<|user|>
{query}</s>
<|assistant|>`);

    const chain = promptTemplate.pipe(model);
    const result = await chain.invoke({ query: query });

    const answer = result
      .substring(result.indexOf("<|assistant|>") + "<|assistant|>".length)
      .trim();
    return answer;
  },
  {
    name: "chatbot", // The name should vary between different functions, and cannot be empty if there are more than one function instances.
  }
);

/**
 * The following code is creating a chain for the question answering task. It can be used to answer
 * the question based on the given context.
 */
const exampleDoc1 = `
Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital.
Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well.
Therefore, Peter stayed with her at the hospital for 3 days without leaving.
`;

const promptTemplate = `Use the following pieces of context to answer the question at the end.

{context}

Question: {question}
Answer:`;

const qaFunc = new Function(
  async (query) => {
    const docs = [new Document({ pageContent: exampleDoc1 })];

    const prompt = new PromptTemplate({
      template: promptTemplate,
      inputVariables: ["context", "question"],
    });

    const chain = loadQAChain(await createSageMakerModel(), {
      type: "stuff",
      prompt: prompt,
    });

    const result = await chain.invoke({ input_documents: docs, question: query });
    return result["text"];
  },
  {
    name: "qa",
  }
);
````

</details>

## å‡†å¤‡å·¥ä½œ

å¦‚æœä½ è¿˜æ²¡æœ‰å®‰è£… Plutoï¼Œè¯·å‚è€ƒ[è¿™é‡Œ](https://github.com/pluto-lang/pluto#-quick-start)çš„æ­¥éª¤å®‰è£… Plutoï¼Œå¹¶é…ç½®å¥½ AWS çš„è®¿é—®å‡­è¯ã€‚

## åˆ›å»ºé¡¹ç›®

é¦–å…ˆï¼Œåœ¨ä½ çš„å·¥ä½œç›®å½•ä¸‹ï¼Œæ‰§è¡Œ `pluto new` å‘½ä»¤ï¼Œè¿™ä¼šäº¤äº’å¼åœ°åˆ›å»ºä¸€ä¸ªæ–°é¡¹ç›®ï¼Œå¹¶åœ¨ä½ å½“å‰ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶å¤¹ï¼Œå…¶ä¸­åŒ…å«äº† Pluto é¡¹ç›®çš„åŸºæœ¬ç»“æ„ã€‚

è¿™é‡Œï¼Œæˆ‘çš„é¡¹ç›®åç§°å‘½åä¸º `langchain-llama2-sagemaker`ï¼Œé€‰æ‹© AWS å¹³å°ï¼Œå¹¶ä¸”ä½¿ç”¨ Pulumi ä½œä¸ºéƒ¨ç½²å¼•æ“ã€‚

```
$ pluto new
? Project name langchain-llama2-sagemaker
? Stack name dev
? Select a platform AWS
? Select an provisioning engine Pulumi
Info:  Created a project, langchain-llama2-sagemaker
```

åˆ›å»ºå®Œæˆåï¼Œè¿›å…¥åˆ›å»ºçš„é¡¹ç›®æ–‡ä»¶å¤¹ `langchain-llama2-sagemaker`ï¼Œä¼šçœ‹åˆ°è¿™æ ·çš„ç›®å½•ç»“æ„ï¼š

```
langchain-llama2-sagemaker/
â”œâ”€â”€ README.md
â”œâ”€â”€ package.json
â”œâ”€â”€ src
â”‚   â””â”€â”€ index.ts
â””â”€â”€ tsconfig.json
```

ç„¶åï¼Œæ‰§è¡Œ `npm install` ä¸‹è½½æ‰€éœ€ä¾èµ–ã€‚

## ç¼–å†™ä»£ç 

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¿®æ”¹ `src/index.ts` æ–‡ä»¶æ¥æ„å»ºæˆ‘ä»¬çš„ç¤ºä¾‹åº”ç”¨ï¼Œè¿‡ç¨‹ä¹Ÿéå¸¸ç®€å•ã€‚

### 1ï¼‰åˆ›å»º SageMaker å®ä¾‹

é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥ `@plutolang/pluto` åŒ…ï¼Œç„¶ååˆ›å»ºä¸€ä¸ª `SageMaker` å®ä¾‹ï¼Œæ¥éƒ¨ç½²æˆ‘ä»¬çš„æ¨¡å‹ã€‚

åœ¨ `SageMaker` æ„é€ å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬éœ€è¦æä¾›åç§°ã€æ¨¡å‹çš„ Docker é•œåƒ URI å’Œä¸€äº›é…ç½®ä¿¡æ¯ï¼Œå…¶ä¸­åç§°ä¸æƒ³è¦éƒ¨ç½²çš„æ¨¡å‹æ²¡æœ‰å…³ç³»ï¼Œåªæ˜¯ç”¨äºç¡®å®š SageMaker å®ä¾‹çš„åç§°ã€‚

````typescript
import { SageMaker, Function } from "@plutolang/pluto";
import { loadQAChain } from "langchain/chains";
import { Document } from "langchain/document";
import { PromptTemplate } from "langchain/prompts";
import {
  SageMakerEndpoint,
  SageMakerLLMContentHandler,
} from "@langchain/community/llms/sagemaker_endpoint";

/**
 * Deploy the Llama2 model on AWS SageMaker using the Hugging Face Text Generation Inference (TGI)
 * container. Here will deploy the TinyLlama-1.1B-Chat-v1.0 model, which can be run on the
 * ml.m5.xlarge instance.
 *
 * Below is a set up minimum requirements for each model size of Llama2 model:
 * ```
 * Model      Instance Type    Quantization    # of GPUs per replica
 * Llama 7B   ml.g5.2xlarge    -               1
 * Llama 13B  ml.g5.12xlarge   -               4
 * Llama 70B  ml.g5.48xlarge   bitsandbytes    8
 * Llama 70B  ml.p4d.24xlarge  -               8
 * ```
 * The initial limit set for these instances is zero. If you need more, you can request an increase
 * in quota via the [AWS Management Console](https://console.aws.amazon.com/servicequotas/home).
 */
const sagemaker = new SageMaker(
  "llama-2-7b",
  "763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.0-gpu-py310-cu121-ubuntu20.04",
  {
    instanceType: "ml.m5.xlarge",
    envs: {
      // HF_MODEL_ID: "meta-llama/Llama-2-7b-chat-hf",
      HF_MODEL_ID: "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
      HF_TASK: "text-generation",
      // If you want to deploy the Meta Llama2 model, you need to request a permission and prepare the
      // token. You can get the token from https://huggingface.co/settings/tokens
      // HUGGING_FACE_HUB_TOKEN: "hf_EmXPwpnyxxxxxxx"
    },
  }
);
````

å¦‚æœä½ æƒ³éƒ¨ç½² Meta å®Œæ•´çš„ Llama2 7Bã€13Bã€70B æ¨¡å‹ï¼Œæœ‰ä¸¤ç‚¹ä½ éœ€è¦æ³¨æ„ï¼š

1. ä¸åŒçš„ Llama2 å¤§è¯­è¨€æ¨¡å‹å¯¹å®ä¾‹çš„è¦æ±‚ä¸åŒï¼Œéœ€è¦é€‰æ‹©ä¸åŒçš„å®ä¾‹ç±»å‹ï¼Œä»¥ä¸‹æ˜¯å„æ¨¡å‹å¯¹åº”çš„æœ€ä½è¦æ±‚ï¼š
   - Llama 7B: `ml.g5.2xlarge`
   - Llama 13B: `ml.g5.12xlarge`
   - Llama 70B: `ml.p4d.24xlarge`
2. ä½ éœ€è¦äº‹å…ˆå‘ Meta è¯·æ±‚ä¸‹è½½æƒé™ï¼Œä½ åœ¨[è¿™ä¸ªç½‘é¡µ](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)åº”è¯¥èƒ½çœ‹åˆ°æç¤ºï¼Œæ ¹æ®æç¤ºå®Œæˆæƒé™ç”³è¯·ã€‚æ­¤å¤–ï¼Œä½ è¿˜éœ€è¦å‡†å¤‡ä¸€ä¸ª Hugging Face çš„ tokenï¼Œä½ å¯ä»¥ä»[è¿™é‡Œ](https://huggingface.co/settings/tokens)è·å–ã€‚

å¦‚æœä½ æƒ³éƒ¨ç½²å…¶ä»–å¤§è¯­è¨€æ¨¡å‹ï¼Œåªéœ€è¦ç¡®å®šä½ è¦éƒ¨ç½²çš„å¤§è¯­è¨€æ¨¡å‹æ”¯æŒ TGI å³å¯ã€‚åœ¨[è¿™é‡Œ](https://huggingface.co/models?other=text-generation-inference)å¯ä»¥æ‰¾åˆ°æ”¯æŒ TGI çš„æ¨¡å‹ã€‚æ‰¾åˆ°éœ€è¦éƒ¨ç½²çš„æ¨¡å‹åï¼Œéœ€è¦å°†æ¨¡å‹çš„ ID å’Œä»»åŠ¡ç±»å‹å¡«å…¥ `envs` ä¸­ã€‚æ¨¡å‹ ID å°±æ˜¯ç½‘é¡µä¸Šæ¨¡å‹çš„åç§°ï¼Œä»»åŠ¡ç±»å‹åˆ™ä½“ç°åœ¨æ¨¡å‹çš„æ ‡ç­¾ä¸­ã€‚

### 2ï¼‰å°† SageMaker éƒ¨ç½²çš„æ¨¡å‹é€‚é…ä¸º LangChain çš„ LLM ç±»å‹

LangChain ç¤¾åŒºä¸­å·²ç»æä¾›äº†ä¸€ä¸ª `SageMakerEndpoint` ç±»ï¼Œç”¨äºå°† SageMaker éƒ¨ç½²çš„æ¨¡å‹é€‚é…ä¸º LangChain æ¥å—çš„ LLM æ¨¡å‹ã€‚æˆ‘ä»¬åªéœ€è¦å®ç° `SageMakerLLMContentHandler` æ¥å£æ¥é€‚é…æ¨¡å‹çš„è¾“å…¥è¾“å‡ºå³å¯ã€‚

`SageMakerEndpoint` æ„é€ å‡½æ•°çš„å‚æ•°åˆ—è¡¨ä¸­åŒ…æ‹¬ `EndpointName`ï¼Œåœ¨åŸºäº Pluto çš„åº”ç”¨ç¨‹åºä¸­ï¼Œæˆ‘ä»¬åªéœ€è¦è°ƒç”¨ `sagemaker.endpointName` å°±å¯è·å–åˆ°ï¼Œä¸éœ€è¦å†å»æ§åˆ¶å°ä¸ŠæŸ¥æ‰¾äº†ã€‚å¹¶ä¸”ï¼Œç”±äºç¼–å†™çš„ä»£ç æœ€ç»ˆä¼šç›´æ¥éƒ¨ç½²æˆ AWS Lambda å®ä¾‹ï¼ŒclientOptions æ‰€éœ€è¦çš„ `region` å‚æ•°ä¹Ÿå¯ä»¥ç›´æ¥ä»ç¯å¢ƒå˜é‡ä¸­è·å–ã€‚

```typescript
async function createSageMakerModel() {
  class LLama27BHandler implements SageMakerLLMContentHandler {
    contentType = "application/json";

    accepts = "application/json";

    async transformInput(prompt: string, modelKwargs: Record<string, unknown>): Promise<any> {
      const payload = {
        inputs: prompt,
        parameters: modelKwargs,
      };
      const stringifiedPayload = JSON.stringify(payload);
      return new TextEncoder().encode(stringifiedPayload);
    }

    async transformOutput(output: any): Promise<string> {
      const response_json = JSON.parse(new TextDecoder("utf-8").decode(output));
      const content: string = response_json[0]["generated_text"] ?? "";
      return content;
    }
  }

  return new SageMakerEndpoint({
    endpointName: sagemaker.endpointName,
    modelKwargs: {
      temperature: 0.5,
      max_new_tokens: 700,
      top_p: 0.9,
    },
    endpointKwargs: {
      CustomAttributes: "accept_eula=true",
    },
    contentHandler: new LLama27BHandler(),
    clientOptions: {
      region: process.env["AWS_REGION"],
    },
  });

  // Cannot be omitted.
  await sagemaker.invoke({});
}
```

çœ‹åˆ°è¿™é‡Œï¼Œä½ æˆ–è®¸ä¼šäº§ç”Ÿä¸€äº›ç–‘é—®ï¼Œ`class` çš„å®šä¹‰ä¸ºä»€ä¹ˆåœ¨å‡½æ•°é‡Œé¢ï¼Ÿ`return` ä¹‹åä¸ºä»€ä¹ˆè¿˜æœ‰ä¸€æ¡è¯­å¥ï¼Ÿè¿™æ˜¯å› ä¸ºå½“å‰ç‰ˆæœ¬çš„ Pluto è¿˜ä¸æˆç†Ÿï¼Œç›®å‰åªèƒ½é€šè¿‡è¿™ç§æ–¹å¼æ¥ç¡®ä¿èƒ½å¤Ÿæ­£ç¡®æ„å»º AWS Lambda å®ä¾‹ã€‚å¦‚æœæœ‰å¤§ä½¬å¯¹è¿™å—åŸç†ä¸å®ç°æ„Ÿå…´è¶£ï¼Œæ¬¢è¿é˜…è¯»[è¿™ç¯‡æ–‡æ¡£](https://pluto-lang.vercel.app/zh-CN/documentation/design/deducer-design)ï¼Œå¹¶ä¸”éå¸¸éå¸¸**æ¬¢è¿ä¸€èµ·å‚ä¸å…±å»º**ã€‚

### 3ï¼‰åˆ›å»ºå¯¹è¯åŠŸèƒ½çš„ Lambda å‡½æ•°

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åŸºäº LangChain çš„ `PromptTemplate` å®ç°æœ€åŸºæœ¬çš„å¯¹è¯åŠŸèƒ½ã€‚

æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª `Function` å¯¹è±¡ `chatFunc`ï¼Œè¿™ä¸ªå¯¹è±¡å¯¹åº”ä¸€ä¸ª AWS Lambda å®ä¾‹ï¼Œè¿™ä¸ªå‡½æ•°æ¥æ”¶ä¸€ä¸ª `query` ä½œä¸ºè¾“å…¥å‚æ•°ï¼Œå¹¶è¿”å›å¤§è¯­è¨€æ¨¡å‹å“åº”çš„ç»“æœã€‚

```typescript
const chatFunc = new Function(
  async (query) => {
    const model = await createSageMakerModel();
    const promptTemplate = PromptTemplate.fromTemplate(`<|system|>
You are a cool and aloof robot, answering questions very briefly and directly.</s>
<|user|>
{query}</s>
<|assistant|>`);

    const chain = promptTemplate.pipe(model);
    const result = await chain.invoke({ query: query });

    const answer = result
      .substring(result.indexOf("<|assistant|>") + "<|assistant|>".length)
      .trim();
    return answer;
  },
  {
    name: "chatbot", // The name should vary between different functions, and cannot be empty if there are more than one function instances.
  }
);
```

è¿™ä¸ªå˜é‡æ˜æ˜åç»­æ²¡æœ‰å†è¢«ä½¿ç”¨ï¼Œå´ä»ä¸èƒ½çœç•¥ï¼ŒåŸå› åŒä¸Š...

### 4ï¼‰åˆ›å»ºé—®ç­”åŠŸèƒ½ Lambda å‡½æ•°

æœ€åï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ª `Function` å¯¹è±¡ `qaFunc`ï¼Œè¿™ä¸ªå¯¹è±¡åŒæ ·å¯¹åº”ä¸€ä¸ª AWS Lambda å®ä¾‹ã€‚è¿™ä¸ªå‡½æ•°æ¥æ”¶ä¸€ä¸ª `query` ä½œä¸ºè¾“å…¥å‚æ•°ï¼Œå¤§è¯­è¨€æ¨¡å‹ä¼šæ ¹æ®é—®é¢˜ä¸è¾“å…¥çš„æ–‡æ¡£å“åº”ç»“æœã€‚

```typescript
const exampleDoc1 = `
Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital.
Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well.
Therefore, Peter stayed with her at the hospital for 3 days without leaving.
`;

const promptTemplate = `Use the following pieces of context to answer the question at the end.

{context}

Question: {question}
Answer:`;

const qaFunc = new Function(
  async (query) => {
    const docs = [new Document({ pageContent: exampleDoc1 })];

    const prompt = new PromptTemplate({
      template: promptTemplate,
      inputVariables: ["context", "question"],
    });

    const chain = loadQAChain(await createSageMakerModel(), {
      type: "stuff",
      prompt: prompt,
    });

    const result = await chain.invoke({ input_documents: docs, question: query });
    return result["text"];
  },
  {
    name: "qa",
  }
);
```

è‡³æ­¤ï¼Œæˆ‘ä»¬çš„ä»£ç å°±å·²ç»ç¼–å†™å®Œæˆï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬åªè¦å°†å…¶éƒ¨ç½²åˆ° AWS ä¸Šï¼Œå°±å¯ä»¥é€šè¿‡ HTTP è¯·æ±‚æ¥è°ƒç”¨æˆ‘ä»¬çš„æ¨¡å‹äº†ã€‚

## ä¸€é”®éƒ¨ç½²

éƒ¨ç½² Pluto é¡¹ç›®ä¹Ÿéå¸¸ç®€å•ï¼Œåªéœ€è¦åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹æ‰§è¡Œ `pluto deploy` å‘½ä»¤ï¼ŒPluto å°±ä¼šè‡ªåŠ¨å°†é¡¹ç›®éƒ¨ç½²åˆ° AWS ä¸Šã€‚éƒ¨ç½²çš„ç»“æœä¼šåƒä¸‹é¢è¿™æ ·ï¼Œå…¶ä¸­çº¢è‰²ä»£è¡¨å¯¹è¯åŠŸèƒ½çš„ Lambda å®ä¾‹ï¼Œç»¿è‰²ä»£è¡¨é—®ç­”åŠŸèƒ½çš„ Lambda å®ä¾‹ã€‚**æ³¨æ„ï¼šSageMaker çš„éƒ¨ç½²æ—¶é—´è¾ƒé•¿ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚**

![alt text](../../assets/langchain-llama2-sagemaker-deployment.png)

![arch](../../assets/langchain-llama2-sagemaker-arch.png)

æ•´ä¸ªåº”ç”¨éƒ¨ç½²åçš„æ¶æ„å°±åƒä¸Šé¢è¿™å¼ å›¾æ‰€å±•ç¤ºçš„ï¼Œæ•´ä½“ä¸Šç”±ä¸€ä¸ª SageMaker å®ä¾‹ã€ä¸¤ä¸ª Lambda å‡½æ•°æ‰€æ„æˆã€‚ä½†æ˜¯ï¼Œåœ¨å®é™…éƒ¨ç½²çš„æ—¶å€™ï¼Œè¿œæ¯”å±•ç¤ºçš„å¤æ‚ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸é…ç½®**å°†è¿‘ 20 ä¸ªé…ç½®é¡¹**ï¼Œå…¶ä¸­å°±åŒ…æ‹¬ SageMaker çš„ Modelã€Endpointï¼ŒLambda å®ä¾‹ï¼Œä»¥åŠå¤šä¸ª IAM è§’è‰²ã€æƒé™ç­‰ã€‚è€Œå¦‚æœä½¿ç”¨ Pluto çš„è¯ï¼Œè¿™æ‰€æœ‰çš„æ“ä½œåªéœ€è¦ä¸€è¡Œå‘½ä»¤å°±å¯ä»¥**è‡ªåŠ¨åŒ–åœ°å®Œæˆ**ã€‚

## åŠŸèƒ½æµ‹è¯•

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°±èƒ½ä½¿ç”¨è¿”å›çš„ URL æ¥è®¿é—®æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºäº†ã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ curl æˆ– Postman å‘ Lambda å‡½æ•°å‘é€ POST HTTP è¯·æ±‚ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¯·æ±‚ä½“éœ€è¦è®¾ç½®æˆä¸€ä¸ªæ•°ç»„çš„å½¢å¼ï¼Œè¿™è¡¨ç¤ºå‡½æ•°å…¥å‚åˆ—è¡¨ã€‚ä¸‹é¢æ˜¯ curl è¯·æ±‚çš„ç¤ºä¾‹ï¼š

```sh
curl -X POST https://<your-lambda-url-id>.lambda-url.<region>.on.aws/ \
  -H "Content-Type: application/json" \
  -d '["What is the capital of France?"]'
```

å¦‚æœä½ æ”¶åˆ°äº†ä¸€ä¸ªå†…å®¹æ˜¯ `{"code":400,"body":"Payload should be an array."}` çš„é”™è¯¯æ¶ˆæ¯ï¼Œä½ å¯ä»¥å°è¯•åœ¨ URL ä¸Šæ·»åŠ ä¸€ä¸ªæŸ¥è¯¢å‚æ•°æ¥è§£å†³ï¼Œä¾‹å¦‚ `https://<your-lambda-url-id>.lambda-url.<region>.on.aws/?n=1`ã€‚ç›®å‰è¿˜ä¸æ¸…æ¥šä¸ºä»€ä¹ˆå‡ºç°è¿™ä¸ªé—®é¢˜ï¼Œå³ä½¿ curl æ—¥å¿—é‡Œæ˜ç¡®æ˜¯ä¸€ä¸ª POST è¯·æ±‚ï¼Œä½†å¦‚æœä¸åŒ…å«æŸ¥è¯¢å‚æ•°çš„è¯ï¼Œåœ¨ Lambda æ—¥å¿—ä¸­å°±å˜æˆäº† GET è¯·æ±‚ã€‚å¦‚æœæœ‰å¤§ä½¬çŸ¥é“å…¶ä¸­åŸå› ï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚

ä¸‹é¢è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†å¯¹è¯åŠŸèƒ½çš„è¯·æ±‚ä¸å“åº”ï¼š

![Chat](../../assets/langchain-llama2-sagemaker-chat.png)

ä¸‹é¢è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†æ–‡æ¡£é—®ç­”åŠŸèƒ½çš„è¯·æ±‚ä¸å“åº”ï¼š

![QA](../../assets/langchain-llama2-sagemaker-qa.png)

ä½ è¿˜å¯ä»¥å°è¯•ä½¿ç”¨ Pluto æä¾›çš„ KVStore æ¥å®ç°ä¸€ä¸ªèƒ½å¤Ÿä¿æŒä¼šè¯çš„å¯¹è¯æœºå™¨äººğŸ¤–ï¸ï¼Œæ¬¢è¿æäº¤ PRï¼

## Q&A

### ä¸ºä»€ä¹ˆä¸ä½¿ç”¨ Routerï¼ˆApi Gatewayï¼‰æ¥å¤„ç†è¯·æ±‚ï¼Ÿ

å› ä¸º ApiGateway è‡ªå¸¦çš„ 30 ç§’è¶…æ—¶é™åˆ¶ï¼Œæ— æ³•è°ƒæ•´ã€‚è¿™æ„å‘³ç€å¦‚æœç”Ÿæˆè¿‡ç¨‹è¶…è¿‡è¿™ä¸ªæ—¶é—´çª—å£ï¼Œæˆ‘ä»¬å°±ä¼šæ”¶åˆ° `503 Service Unavailable` çš„é”™è¯¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨ Lambda å‡½æ•°æ¥å¤„ç†è¯·æ±‚ã€‚åç»­ä¼šå°è¯•é€šè¿‡æ”¯æŒ WebSocket æ¥æå‡ä½“éªŒã€‚
