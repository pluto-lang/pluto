---
title: åŸºäº Llama3 çš„æ–‡æ¡£é—®ç­”æœºå™¨äºº
description: æœ¬æ–‡ä»‹ç»å¦‚ä½•ä½¿ç”¨ Pluto å¼€å‘å¹¶éƒ¨ç½²ä¸€ä¸ªåŸºäº Llama3ã€LangChain çš„æ–‡æ¡£é—®ç­”æœºå™¨äººï¼Œå¯ä»¥æŒ‡å®šä¸€ä¸ª GitHub ä»“åº“ï¼Œç„¶åä½¿ç”¨ Llama3 æ¨¡å‹æ ¹æ®æ–‡æ¡£å†…å®¹è¿›è¡Œé—®ç­”ã€‚
tags: ["Python", "AWS", "SageMaker", "LangChain", "Llama3"]
---

# åŸºäº Llama3 çš„æ–‡æ¡£é—®ç­”æœºå™¨äºº

> Llama3 æ˜¯ç”± Meta å…¬å¸æœ€æ–°æ¨å‡ºçš„ä¸€æ¬¾å¼€æºçš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ŒåŒ…å« 80 äº¿å‚æ•°çš„ Llama3 8B å’ŒåŒ…å« 700 äº¿å‚æ•°çš„ Llama3 70Bã€‚Llama3 åœ¨æ€§èƒ½ä¸Šå®ç°äº†é‡å¤§è¿›æ­¥ï¼Œ8B æ¨¡å‹åœ¨ MMLUã€GPQAã€HumanEval ç­‰å¤šé¡¹åŸºå‡†ä¸Šå‡èƒœè¿‡ Gemma 7B å’Œ Mistral 7B Instructï¼Œè€Œ 70B æ¨¡å‹åˆ™è¶…è¶Šäº†é—­æºçš„ Claude 3 Sonnetï¼Œä¸è°·æ­Œçš„ Gemini Pro 1.5 æ——é¼“ç›¸å½“ã€‚æ­¤å¤–ï¼ŒMeta è¿˜åœ¨ç ”å‘ä¸€ä¸ªå‚æ•°è§„æ¨¡è¶…è¿‡ 4000 äº¿ï¼ˆ400Bï¼‰çš„ç‰ˆæœ¬ï¼Œé¢„è®¡å°†å…·æœ‰æ›´å¼ºå¤§çš„å¤šè¯­è¨€å¤„ç†èƒ½åŠ›ä»¥åŠç†è§£å›¾åƒç­‰éæ–‡æœ¬æ¨¡å¼çš„èƒ½åŠ›ã€‚

åŸºäº Llama3 æˆ‘ä»¬å¯ä»¥å¼€å‘èŠå¤©æœºå™¨äººã€RAG (Retrieval-Augmented Generationï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆ) é—®ç­”æœºå™¨äººç­‰å„ç±»åº”ç”¨ã€‚ä½†æ˜¯ï¼Œå¦‚ä½•å°† Llama3 éƒ¨ç½²èµ·æ¥ï¼Œå¦‚ä½•å°†éƒ¨ç½²åçš„ Llama3 ä¸è‡ªå·±çš„åº”ç”¨ç¨‹åºç»“åˆï¼Œå¦‚ä½•éƒ¨ç½²åº”ç”¨ç¨‹åºï¼Œè¿™äº›é—®é¢˜å¯¹äºå¾ˆå¤šå¼€å‘è€…æ¥è¯´ä»æœ‰ä¸€å®šçš„éš¾åº¦ã€‚

è¿™ç¯‡æ–‡ç« ä»‹ç»ä¸€ç§åŸºäº [Pluto](https://github.com/pluto-lang/pluto) çš„å¼€å‘æ–¹å¼ï¼Œåªéœ€è¦ç¼–å†™åº”ç”¨ä»£ç ï¼Œç„¶å**æ‰§è¡Œä¸€æ¡å‘½ä»¤å°±å¯ä»¥å®Œæˆ Llama3 çš„éƒ¨ç½²å’Œåº”ç”¨çš„å‘å¸ƒ**ã€‚æœ¬æ–‡ä¼šä»¥ä¸€ä¸ªåŸºäº RAG çš„æ–‡æ¡£é—®ç­”æœºå™¨äººä¸ºä¾‹æ¥ä»‹ç»è¿™ç§å¼€å‘æ–¹å¼ï¼Œè¿™ä¸ªé—®ç­”æœºå™¨äººä¸»è¦åŠŸèƒ½æ˜¯ä»æŒ‡å®š GitHub ä»“åº“ä¸­è·å–é¡¹ç›®æ–‡æ¡£ï¼Œç„¶åä½¿ç”¨ Llama3 æ¨¡å‹æ ¹æ®æ–‡æ¡£å†…å®¹è¿›è¡Œé—®ç­”ã€‚

ä¸‹é¢è¿™å¹…å›¾æ˜¯å’Œè¿™ä¸ªé—®ç­”æœºå™¨äººäº¤äº’çš„æ•ˆæœï¼Œç»™è¿™ä¸ªæœºå™¨äººæŒ‡å®šçš„ä»“åº“æ˜¯ Pluto çš„æ–‡æ¡£ä»“åº“ï¼Œå› æ­¤ï¼Œå¤§å®¶å¯ä»¥ä»å›¾ç‰‡ä¸­çš„å†…å®¹ç®€å•äº†è§£ Pluto æ˜¯ä»€ä¹ˆï¼š

![Example Output](./assets/doc-qa-bot-result-cn.png)

## åº”ç”¨æ¶æ„

æ¥ä¸‹æ¥è¦å®ç°çš„ç¤ºä¾‹åº”ç”¨åŸºäº LangChain æ¡†æ¶å®ç°ï¼Œä½¿ç”¨ OpenAI Embeddings ä½œä¸ºæ–‡æ¡£çš„å‘é‡åŒ–å·¥å…·ã€‚æœ€ç»ˆæ•´ä¸ªåº”ç”¨ä¼šéƒ¨ç½²åˆ° AWS ä¸Šï¼Œéƒ¨ç½²åçš„æ¶æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![Arch](./assets/doc-qa-bot-arch.png)

å…·ä½“åœ°ï¼Œè¿™ä¸ªåº”ç”¨éƒ¨ç½²åä¼šåŒ…æ‹¬ä»¥ä¸‹ AWS èµ„æºå®ä¾‹ï¼š

- Llama3 æ¨¡å‹ä¼šè¢«éƒ¨ç½²åˆ° SageMaker ä¸Šã€‚
- åˆ›å»ºä¸€ä¸ª S3 Bucket ç”¨äºå­˜å‚¨æ–‡æ¡£çš„å‘é‡æ•°æ®åº“ï¼Œé¿å…æ¯æ¬¡å¯åŠ¨ Lambda éƒ½æ„å»ºä¸€æ¬¡å‘é‡æ•°æ®åº“ã€‚
- åˆ›å»ºä¸€ä¸ª CloudWatch è§„åˆ™ï¼Œç”¨äºæ¯å¤©æ›´æ–°ä¸€æ¬¡æ–‡æ¡£çš„å‘é‡æ•°æ®åº“ã€‚
- åˆ›å»ºä¸¤ä¸ª Lambda å®ä¾‹ï¼Œåˆ†åˆ«ç”¨äºæ¥æ”¶ç”¨æˆ·çš„æŸ¥è¯¢è¯·æ±‚ï¼Œä»¥åŠæ›´æ–°æ–‡æ¡£çš„å‘é‡æ•°æ®åº“ã€‚

é™¤äº†åˆ›å»ºè¿™äº›èµ„æºå®ä¾‹å¤–ï¼Œè¿˜éœ€è¦é…ç½®èµ„æºé—´çš„ä¾èµ–å…³ç³»ï¼ŒåŒ…æ‹¬è§¦å‘å™¨ã€IAM è§’è‰²å’Œæƒé™ç­–ç•¥ç­‰ã€‚ä¸è¿‡ï¼Œä½ ä¸éœ€è¦æ‹…å¿ƒè¿™äº›ç¹ççš„åˆ›å»ºä¸é…ç½®è¿‡ç¨‹ï¼Œ**Pluto å¯ä»¥ä»ä»£ç ä¸­æ¨å¯¼å‡ºæ¥è¿™äº›ä¿¡æ¯ï¼Œç„¶åè‡ªåŠ¨å®Œæˆèµ„æºçš„åˆ›å»ºä¸é…ç½®**ã€‚

## å‡†å¤‡å¼€å‘ç¯å¢ƒä¸ Token

### å‡†å¤‡å¼€å‘ç¯å¢ƒ

é¦–å…ˆéœ€è¦å‡†å¤‡ Pluto åº”ç”¨çš„å¼€å‘ç¯å¢ƒï¼ŒPluto æä¾›äº†å®¹å™¨å¼€å‘ã€åœ¨çº¿å¼€å‘ã€æœ¬åœ°å¼€å‘ä¸‰ç§ä¸åŒçš„å¼€å‘æ–¹å¼ï¼š

- å®¹å™¨å¼€å‘ï¼šåŸºäº `plutolang/pluto:latest` å®¹å™¨é•œåƒåˆ›å»ºä¸€ä¸ªå®¹å™¨ä½œä¸º Pluto åº”ç”¨çš„å¼€å‘ç¯å¢ƒã€‚
- åœ¨çº¿å¼€å‘ï¼šæ‰“å¼€å¹¶ Fork åœ¨ CodeSandbox ä¸Šåˆ›å»ºçš„[æ¨¡æ¿åº”ç”¨](https://codesandbox.io/p/devbox/github/pluto-lang/codesandbox/tree/main/python?file=/README_zh.md)ï¼Œå°±å¯ä»¥ç›´æ¥åœ¨æµè§ˆå™¨ä¸­å¼€å‘åº”ç”¨ã€‚
- æœ¬åœ°å¼€å‘ï¼šå‚è€ƒ [Pluto æœ¬åœ°å¼€å‘æŒ‡å—](https://pluto-lang.vercel.app/zh-CN/documentation/getting-started/local-development) åœ¨æœ¬åœ°é…ç½® Pluto å¼€å‘ç¯å¢ƒã€‚

å¯ä»¥ä» [Pluto ä¸Šæ‰‹æŒ‡å—](https://pluto-lang.vercel.app/zh-CN/documentation/getting-started) ä¸­äº†è§£å„ç§ç¯å¢ƒè¯¦ç»†çš„ä½¿ç”¨æ–¹æ³•ã€‚

### å‡†å¤‡ AWS èµ„æºé…é¢

éƒ¨ç½² Llama3 8B æ¨¡å‹æ‰€éœ€çš„å®ä¾‹ç±»å‹è‡³å°‘ä¸º `ml.g5.2xlarge`ï¼Œéƒ¨ç½² Llama3 70B æ¨¡å‹æ‰€éœ€çš„å®ä¾‹ç±»å‹è‡³å°‘ä¸º `ml.p4d.24xlarge`ã€‚è¿™ä¸¤ç±»å®ä¾‹çš„åˆå§‹é…é¢ä¸ºé›¶ï¼Œæ‰€ä»¥ï¼Œå¦‚æœä½ æ²¡æœ‰ç”³è¯·è¿‡ï¼Œå¯èƒ½éœ€è¦é€šè¿‡ [AWS ç®¡ç†æ§åˆ¶å°](https://console.aws.amazon.com/servicequotas/home) ç”³è¯·æé¢ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨ `TinyLlama-1.1B-Chat-v1.0` æ¨¡å‹ä½“éªŒï¼Œè¯¥æ¨¡å‹ä¸ `ml.m5.xlarge` å®ä¾‹å…¼å®¹ã€‚

### å‡†å¤‡ Token

æœ¬ç¤ºä¾‹åº”ç”¨éœ€è¦å‡†å¤‡ä»¥ä¸‹å‡ ä¸ªä¸åŒç½‘ç«™çš„ API Key æ¥å®ç°åº”ç”¨åŠŸèƒ½ï¼š

1. OpenAI API Keyï¼šç”¨äºè®¿é—® OpenAI Embeddings APIã€‚ä½ å¯ä»¥ä» [OpenAI](https://platform.openai.com/account/api-keys) è·å– API Keyã€‚
2. GitHub Access Tokenï¼šç”¨äºä» GitHub ä»“åº“è·å–æ–‡æ¡£ã€‚ä½ å¯ä»¥ä» [GitHub](https://github.com/settings/tokens) åˆ›å»ºä¸ªäººè®¿é—®ä»¤ç‰Œã€‚
3. Hugging Face Hub Tokenï¼šç”¨äºåœ¨ AWS SageMaker ä¸Šéƒ¨ç½²æ¨¡å‹æ—¶ä» Hugging Face Hub ä¸‹è½½æ¨¡å‹ã€‚ä½ å¯ä»¥ä» [Hugging Face](https://huggingface.co/settings/tokens) è·å– Tokenã€‚

## åˆ›å»º Pluto åº”ç”¨

å¦‚æœæ˜¯æœ¬åœ°æˆ–å®¹å™¨ç¯å¢ƒï¼Œæ‰§è¡Œä¸‹é¢è¿™æ¡å‘½ä»¤äº¤äº’å¼åœ°åˆ›å»ºä¸€ä¸ªæ–°çš„ Pluto åº”ç”¨ï¼Œç»“æŸåä¼šåœ¨å½“å‰ç›®å½•ä»¥é¡¹ç›®ååˆ›å»ºä¸€ä¸ªæ–°çš„é¡¹ç›®ç›®å½•ã€‚

```bash
pluto new
```

äº‘ç«¯å¼€å‘ç¯å¢ƒä¸­å·²ç»åŒ…å«äº†åŸºæœ¬çš„é¡¹ç›®ç›®å½•ï¼Œæ— éœ€æ–°å»ºåº”ç”¨ã€‚

## å®‰è£…åº”ç”¨ä¾èµ–

è¿›å…¥é¡¹ç›®æ ¹ç›®å½•åï¼Œåœ¨ `requirements.txt` æ–‡ä»¶ä¸­æ·»åŠ ä»¥ä¸‹ä¾èµ–åº“ï¼Œè¿™äº›ä¾èµ–åº“æ˜¯æœ¬ç¤ºä¾‹åº”ç”¨æ‰€éœ€çš„ä¾èµ–åº“ï¼š

```txt
pluto_client
faiss-cpu
langchain-core
langchain-community
langchain-openai
langchain_text_splitters
```

ç„¶åæ‰§è¡Œä¸‹é¢è¿™ä¸¤æ¡å‘½ä»¤å®‰è£…ä¾èµ–åº“ï¼š

```bash
npm install
pip install -r requirements.txt
```

## ç¼–å†™åº”ç”¨ä»£ç 

å®‰è£…å®Œä¾èµ–åï¼Œå°±å¯ä»¥åœ¨ `app/main.py` æ–‡ä»¶ä¸­ç¼–å†™åº”ç”¨ç¨‹åºäº†ï¼Œæœ¬æ–‡æœ€åé™„äº†ä¸€ä»½æ–‡æ¡£é—®ç­”æœºå™¨äººçš„[ç¤ºä¾‹ä»£ç ](#ç¤ºä¾‹ä»£ç )ï¼Œä½ å¯ä»¥ç›´æ¥å¤åˆ¶åˆ° `app/main.py` æ–‡ä»¶ä¸­ï¼Œç„¶åä¿®æ”¹å…¶ä¸­çš„é…ç½®å‚æ•°å³å¯ã€‚

ç¼–å†™ Pluto åº”ç”¨ä»£ç ä¸ç¼–å†™çº¯ä¸šåŠ¡ä»£ç ç±»ä¼¼ï¼Œå¼€å‘è€…ä¸éœ€è¦å…³å¿ƒäº‘èµ„æºæ˜¯æ€ä¹ˆè¢«åˆ›å»ºå’Œé…ç½®çš„ï¼Œåªéœ€è¦åœ¨ä»£ç ä¸­é€šè¿‡åˆ›å»ºå¯¹è±¡çš„æ–¹å¼ï¼Œå®šä¹‰åº”ç”¨éœ€è¦çš„èµ„æºï¼Œä»¥åŠå®ç°åº”ç”¨ç¨‹åºçš„ä¸šåŠ¡é€»è¾‘ã€‚åœ¨éƒ¨ç½²æ—¶ï¼ŒPluto ä¼šè‡ªåŠ¨ä»åº”ç”¨é€»è¾‘ä¸­æ¨å¯¼å‡ºäº‘èµ„æºé—´çš„ä¾èµ–å…³ç³»ï¼Œå¹¶åœ¨äº‘å¹³å°åˆ›å»ºå’Œé…ç½®è¿™äº›èµ„æºã€‚

ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸‹é¢è¿™æ®µä»£ç æ¥å®šä¹‰ä¸€ä¸ªéƒ¨ç½²äº† Llama3 æ¨¡å‹çš„ SageMaker ç«¯ç‚¹ï¼š

```python
sagemaker = SageMaker(
    "llama3-model",
    "763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi2.0.0-gpu-py310-cu121-ubuntu22.04-v2.0",
    SageMakerOptions(
        instanceType="ml.g5.2xlarge",
        envs={
            "HF_MODEL_ID": "meta-llama/Meta-Llama-3-8B-Instruct",
            "HF_TASK": "text-generation",
            "HUGGING_FACE_HUB_TOKEN": HUGGING_FACE_HUB_TOKEN,
        },
    ),
)
```

åœ¨åç»­çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è°ƒç”¨ `sagemaker.invoke()` æ–¹æ³•æ¥è§¦å‘ SageMaker ç«¯ç‚¹ï¼Œæˆ–é€šè¿‡ `sagemaker.endpointName` è·å– SageMaker ç«¯ç‚¹çš„åç§°ï¼ŒPluto ä¼šè‡ªåŠ¨ä¸ºè°ƒç”¨çš„ SageMaker å®ä¾‹çš„å‡½æ•°è®¾ç½®æ­£ç¡®çš„æƒé™ï¼Œä»¥ä¾¿è°ƒç”¨ SageMaker ç«¯ç‚¹ã€‚

åŒç†ï¼Œåˆ›å»ºä¸€ä¸ª S3ã€Lambda ç­‰èµ„æºä¹Ÿæ˜¯ä¸€æ ·ï¼Œä¸åŒçš„æ˜¯ï¼ŒPluto é’ˆå¯¹æ­¤ç±»å¸¸ç”¨ä¸”å¤§å¤šæ•°äº‘å¹³å°éƒ½æä¾›çš„èµ„æºç±»å‹ï¼ŒPluto æä¾›äº†ä¸€å±‚æŠ½è±¡ç±»å‹æ¥é™ä½å¼€å‘è€…çš„å­¦ä¹ æˆæœ¬ï¼ŒåŒæ—¶ä¹Ÿä¾¿äºå®ç°å¤šäº‘éƒ¨ç½²çš„ç‰¹æ€§â€”â€”**ä¸ä¿®æ”¹ä»£ç å³å¯ç›´æ¥éƒ¨ç½²åˆ°å¤šä¸ªäº‘å¹³å°**ã€‚

ä¸‹é¢ä»£ç ä¸­ï¼ŒFunction èµ„æºç±»å‹åœ¨ AWS ä¸Šå¯¹åº” Lambda å‡½æ•°ï¼Œåœ¨ Kubernetes ä¸Šå¯¹åº” Knative Serviceï¼Œè€Œ Bucket èµ„æºç±»å‹åœ¨ AWS ä¸Šå¯¹åº” S3 å­˜å‚¨æ¡¶ï¼Œåœ¨ Kubernetes ä¸Šçš„å¯¹åº”ç±»å‹é¢„æœŸä¸º PVï¼Œè¿˜æœªå®ç°ï¼Œæ¬¢è¿å¤§å®¶å‚ä¸è´¡çŒ®ã€‚

```python
vector_store_bucket = Bucket("vector-store")
Function(query, FunctionOptions(name="query", memory=512))
```

## éƒ¨ç½²åº”ç”¨

åœ¨å®Œæˆåº”ç”¨ä»£ç ç¼–å†™åï¼Œåªéœ€è¦æ‰§è¡Œä¸‹é¢è¿™æ¡å‘½ä»¤å°±å¯ä»¥ç›´æ¥å°†åº”ç”¨éƒ¨ç½²åˆ° AWS ä¸Šï¼š

```bash
pluto deploy
```

åˆ›å»º SageMaker çš„è¿‡ç¨‹å¯èƒ½éå¸¸æ¼«é•¿ï¼Œç”šè‡³è¶…è¿‡ 20 åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚éƒ¨ç½²å®Œæˆåï¼ŒPluto ä¼šè¿”å›ä¸€ä¸ª URL åœ°å€ï¼Œä½ å¯ä»¥é€šè¿‡è¿™ä¸ª URL åœ°å€æ¥è®¿é—®ä½ çš„åº”ç”¨ã€‚

## æµ‹è¯•åº”ç”¨

è¿™ä¸ª URL åœ°å€æ¥å— POST è¯·æ±‚ï¼Œè¯·æ±‚ä½“è¦æ±‚æ˜¯ä¸€ä¸ª JSON æ•°ç»„ï¼Œæ•°ç»„ä¸­åªèƒ½æœ‰ä¸€ä¸ªå…ƒç´ ï¼Œå¯¹åº”ä»£ç ä¸­ `query` å‡½æ•°çš„å‚æ•°ã€‚ä½ å¯ä»¥ä½¿ç”¨ `curl` å‘½ä»¤æˆ–è€…å…¶ä»– HTTP å®¢æˆ·ç«¯å·¥å…·æ¥ä¸ä½ çš„åº”ç”¨è¿›è¡Œäº¤äº’ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥å°†ä¸‹é¢è¿™æ®µä»£ç ä¿å­˜ä¸­çš„ `URL` æ›¿æ¢ä¸ºä½ å¾—åˆ°çš„ URL åœ°å€ï¼Œå…·ä½“é—®é¢˜ä¹Ÿå¯ä»¥æ›¿æ¢ä¸ºä½ æƒ³é—®çš„é—®é¢˜ï¼Œç„¶åæ‰§è¡Œè¿™æ®µä»£ç ï¼Œå°±å¯ä»¥ä¸ä½ çš„åº”ç”¨è¿›è¡Œäº¤äº’äº†ï¼š

```bash
curl -X POST <URL> \
    -H "Content-Type: application/json" \
    -d '["What is Pluto?"]'
```

è¿™é‡Œæä¾›ä¸€ä¸ªç®€å•çš„äº¤äº’å¼è„šæœ¬ï¼Œä½ å¯ä»¥å°†è¿™ä¸ªè„šæœ¬ä¿å­˜åˆ°ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼Œç„¶åæ‰§è¡Œè¿™ä¸ªæ–‡ä»¶æ¥ä¸ä½ çš„åº”ç”¨è¿›è¡Œäº¤äº’ï¼Œå°±èƒ½è·å¾—æœ¬æ–‡åœ¨æœ€å¼€å§‹å±•ç¤ºçš„æ•ˆæœäº†ã€‚

```bash
#!/bin/bash
# set -o xtrace

echo "NOTE: This QA bot cannot keep a conversation. It can only answer the question you just asked."
echo ""

read -p "Input the URL that Pluto has outputted: " URL
if [ -z $URL ]; then
    echo "Please set the BOT_URL env var first"
    exit 1
fi

echo -e "\nNow you can ask your question!"
user_message=""
while :; do
    echo "Press 'q' to quit."
    read -p "User > " user_message
    if [[ $user_message == "q" ]]; then
        echo "Bye. ğŸ‘‹"
        break
    fi

    payload=$(jq -n --arg msg "$user_message" '[$msg]')
    response=$(curl -s -w "\n%{http_code}" -X POST "$URL?n=1" -d "$payload" -H 'Content-type: application/json')

    http_code=$(echo "$response" | tail -n1)
    response_content=$(echo "$response" | head -n-1)
    body=$(echo "$response_content" | jq -r '.body')
    code=$(echo "$response_content" | jq -r '.code')

    if [[ "$http_code" -ne 200 || "$code" -ne 200 ]]; then
        echo "Server responded with error: $http_code, $response_content"
        exit 1
    fi

    body=$(echo "$response_content" | jq -r '.body')
    echo "Bot  > $body"
    echo -e "\n"
done
```

## ä¸‹çº¿åº”ç”¨

å¦‚æœä½ æƒ³ä¸‹çº¿åº”ç”¨ï¼Œåªéœ€è¦æ‰§è¡Œä¸‹é¢è¿™æ¡å‘½ä»¤ï¼š

```bash
pluto destroy
```

## æ‰©å±•åº”ç”¨

å¦‚æœä½ æƒ³å®ç°**åŸºäºä¼šè¯çš„é—®ç­”æœºå™¨äºº**ï¼Œä½ å¯ä»¥ä½¿ç”¨ KVStore èµ„æºç±»å‹æ¥ä¿å­˜ä¼šè¯ï¼Œç¤ºä¾‹åº”ç”¨ã€Œ[ä¼šè¯èŠå¤©æœºå™¨äºº](https://pluto-lang.vercel.app/zh-CN/cookbook/langchain-llama2-chatbot-sagemaker-python)ã€å¯åšå‚è€ƒã€‚

å¦‚æœä½ æƒ³å°†è¯¥åº”ç”¨**æ”¹å†™æˆ LangServe åº”ç”¨**ï¼Œä»¥ä½¿ç”¨ LangServe çš„ RemoteRunableã€Playground ç»„ä»¶ï¼Œä½ å¯ä»¥å‚è€ƒã€Œ[éƒ¨ç½² LangServe åˆ° AWS](https://pluto-lang.vercel.app/zh-CN/cookbook/deploy-langserve-to-aws)ã€æ–‡æ¡£

## æ›´å¤šèµ„æº

- [Pluto æ–‡æ¡£](https://pluto-lang.vercel.app/zh-CN/)
- [Pluto GitHub ä»“åº“](https://github.com/pluto-lang/pluto)
- [Pluto æ¡ˆä¾‹é›†](https://pluto-lang.vercel.app/zh-CN/cookbook)

## ç¤ºä¾‹ä»£ç 

ä»¥ä¸‹æ˜¯æ–‡æ¡£é—®ç­”æœºå™¨äººçš„ç¤ºä¾‹ä»£ç ã€‚ä½ å¯ä»¥å°†å®ƒå¤åˆ¶åˆ° `app/main.py` æ–‡ä»¶ä¸­ï¼Œå¹¶æ ¹æ®éœ€è¦ä¿®æ”¹é…ç½®å‚æ•°ã€‚

```python
import os
import re
import sys
import json
import logging
from typing import Dict

from pluto_client import FunctionOptions, Function, Bucket, Schedule
from pluto_client.sagemaker import SageMaker, SageMakerOptions

from langchain_openai import OpenAIEmbeddings
from langchain_core.pydantic_v1 import SecretStr
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores.faiss import FAISS
from langchain_text_splitters import MarkdownTextSplitter
from langchain_community.document_loaders.github import GithubFileLoader
from langchain_community.llms.sagemaker_endpoint import (
    SagemakerEndpoint,
    LLMContentHandler,
)


# ====== Configuration ======
# 1. The OpenAI API key is used to access the OpenAI Embeddings API. You can get the API key from
# https://platform.openai.com/account/api-keys
# 2. The GitHub Access Key is used to fetch the documents from the GitHub repository. You can create
# a personal access token from https://github.com/settings/tokens
# 3. The Hugging Face Hub token is used to download the model from the Hugging Face Hub when
# deploying the model on AWS SageMaker. You can get the token from
# https://huggingface.co/settings/tokens

REPO = "pluto-lang/website"
BRANCH = "main"
DOC_RELATIVE_PATH = "pages"
OPENAI_BASE_URL = "https://api.openai.com/v1"
OPENAI_API_KEY = "<replace_with_your_key>"
GITHUB_ACCESS_KEY = "<replace_with_your_key>"
HUGGING_FACE_HUB_TOKEN = "<replace_with_your_key>"
# ===========================


FAISS_INDEX = "index"
PKL_KEY = f"{FAISS_INDEX}.pkl"
FAISS_KEY = f"{FAISS_INDEX}.faiss"

embeddings = OpenAIEmbeddings(
    base_url=OPENAI_BASE_URL, api_key=SecretStr(OPENAI_API_KEY)
)

vector_store_bucket = Bucket("vector-store")

"""
Deploy the Llama3 model on AWS SageMaker using the Hugging Face Text Generation Inference (TGI)
container. If you're unable to deploy the model because of the instance type, consider using the
TinyLlama-1.1B-Chat-v1.0 model, which is compatible with the ml.m5.xlarge instance.

Below is a set up minimum requirements for each model size of Llama3 model:
Model      Instance Type      # of GPUs per replica
Llama 8B   ml.g5.2xlarge      1
Llama 70B  ml.p4d.24xlarge    8

The initial limit set for these instances is zero. If you need more, you can request an increase
in quota via the [AWS Management Console](https://console.aws.amazon.com/servicequotas/home).
"""
sagemaker = SageMaker(
    "llama3-model",
    "763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi2.0.0-gpu-py310-cu121-ubuntu22.04-v2.0",
    SageMakerOptions(
        instanceType="ml.g5.2xlarge",
        envs={
            "HF_MODEL_ID": "meta-llama/Meta-Llama-3-8B-Instruct",
            "HF_TASK": "text-generation",
            # If you want to deploy the Meta Llama3 model, you need to request a permission and
            # prepare the token. You can get the token from https://huggingface.co/settings/tokens
            "HUGGING_FACE_HUB_TOKEN": HUGGING_FACE_HUB_TOKEN,
        },
    ),
)


class ContentHandler(LLMContentHandler):
    content_type = "application/json"
    accepts = "application/json"

    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:
        if "stop" not in model_kwargs:
            model_kwargs["stop"] = ["<|eot_id|>"]
        elif "<|eot_id|>" not in model_kwargs["stop"]:
            model_kwargs["stop"].append("<|eot_id|>")

        input_str = json.dumps({"inputs": prompt, "parameters": model_kwargs})
        return input_str.encode("utf-8")

    def transform_output(self, output: bytes) -> str:
        raw = output.read()  # type: ignore
        response_json = json.loads(raw.decode("utf-8"))
        content = response_json[0]["generated_text"]

        assistant_beg_flag = "assistant<|end_header_id|>"
        answerStartPos = content.index(assistant_beg_flag) + len(assistant_beg_flag)
        answer = content[answerStartPos:].strip()
        return answer


def build_logger():
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)
    # Create a console handler
    handler = logging.StreamHandler()
    handler.flush = sys.stdout.flush
    handler.setLevel(logging.INFO)
    # Create a formatter and add it to the handler
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    handler.setFormatter(formatter)
    # Add the handler to the logger
    logger.addHandler(handler)
    return logger


logger = build_logger()


def create_vector_store() -> FAISS | None:
    # Explicitly import Faiss to alert Pluto that this function relies on it, ensuring the inclusion
    # of the Faiss package in the deployment bundle.
    import faiss

    def file_filter(file_path):
        return re.match(f"{DOC_RELATIVE_PATH}/.*\\.mdx?", file_path) is not None

    loader = GithubFileLoader(
        repo=REPO,
        branch=BRANCH,
        access_token=GITHUB_ACCESS_KEY,
        github_api_url="https://api.github.com",
        file_filter=file_filter,
    )
    docs = loader.load()

    if len(docs) == 0:
        logger.info("No documents updated")
        return
    logger.info(f"Loaded {len(docs)} documents")

    for doc in docs:
        doc.metadata["source"] = str(doc.metadata["source"])

    logger.info(f"Starting to split documents")
    text_splitter = MarkdownTextSplitter(chunk_size=2000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)

    logger.info(f"Starting to create vector store")
    store = FAISS.from_documents(splits, embeddings)
    logger.info(f"Finished creating vector store")

    return store


def download_vector_store(vector_store_dir: str):
    ensure_dir(vector_store_dir)
    vector_store_bucket.get(PKL_KEY, os.path.join(vector_store_dir, PKL_KEY))
    vector_store_bucket.get(FAISS_KEY, os.path.join(vector_store_dir, FAISS_KEY))


def flush_vector_store(vector_store_dir: str = "/tmp/vector_store"):
    vector_store = create_vector_store()
    if vector_store is None:
        return

    ensure_dir(vector_store_dir)
    vector_store.save_local(vector_store_dir, index_name=FAISS_INDEX)
    vector_store_bucket.put(PKL_KEY, os.path.join(vector_store_dir, PKL_KEY))
    vector_store_bucket.put(FAISS_KEY, os.path.join(vector_store_dir, FAISS_KEY))


def build_retriever():
    vector_store_dir = "/tmp/vector_store"
    if not os.path.exists(vector_store_dir):
        try:
            logger.info("Vector store not found, downloading...")
            download_vector_store(vector_store_dir)
        except Exception as e:
            logger.error(f"Failed to download vector store: {e}")
            flush_vector_store(vector_store_dir)

    logger.info("Loading vector store")
    vectorstore = FAISS.load_local(
        vector_store_dir, embeddings, allow_dangerous_deserialization=True
    )
    logger.info("Vector store loaded")
    return vectorstore.as_retriever()


def ensure_dir(dir: str):
    if not os.path.exists(dir):
        os.makedirs(dir)


def get_aws_region() -> str:
    aws_region = os.environ.get("AWS_REGION")
    if aws_region is None:
        raise ValueError("AWS_REGION environment variable must be set")
    return aws_region


# Leaving the following variable outside the handler function will allow them to be reused across
# multiple invocations of the function.
retriever = build_retriever()

# Create the prompt template in accordance with the structure provided in the Llama3 documentation,
# which can be found at https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/
prompt = PromptTemplate.from_template(
    """<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. In case the query requests a link, respond that you don't support links.
Context: {context}<|eot_id|><|start_header_id|>user<|end_header_id|>

{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>"""
)

llm = SagemakerEndpoint(
    endpoint_name=sagemaker.endpoint_name,  # SageMaker endpoint name
    region_name=get_aws_region(),
    content_handler=ContentHandler(),
    model_kwargs={
        "max_new_tokens": 512,
        "do_sample": True,
        "temperature": 0.6,
        "top_p": 0.9,
    },
)


def query(query):
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return rag_chain.invoke(query)
    # The line below serves as a notification to Pluto that the function will trigger SageMaker
    # endpoint. So, Pluto will set the appropriate permissions for the function.
    sagemaker.invoke("")


schd = Schedule("schedule")
schd.cron("0 0 * * *", flush_vector_store)

# This application requires a minimum of 256MB memory to run.
Function(query, FunctionOptions(name="query", memory=512))
```

## Q&A

### ä¸ºä»€ä¹ˆä¸ä½¿ç”¨ Api Gatewayï¼Ÿ

æœ‰ä¸¤ä¸ªåŸå› ï¼š

1. ApiGateway è‡ªå¸¦çš„ 30 ç§’è¶…æ—¶é™åˆ¶ï¼Œæ— æ³•è°ƒæ•´ã€‚è¿™æ„å‘³ç€å¦‚æœç”Ÿæˆè¿‡ç¨‹è¶…è¿‡è¿™ä¸ªæ—¶é—´çª—å£ï¼Œæˆ‘ä»¬å°±ä¼šæ”¶åˆ° `503 Service Unavailable` çš„é”™è¯¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨ Lambda å‡½æ•°æ¥å¤„ç†è¯·æ±‚ã€‚åç»­ä¼šå°è¯•é€šè¿‡æ”¯æŒ WebSocket æ¥æå‡ä½“éªŒã€‚
2. è¿™ä¸ªç¤ºä¾‹åº”ç”¨æ‰€éœ€çš„å†…å­˜è¶…è¿‡é»˜è®¤çš„ 128MBï¼Œè€Œé’ˆå¯¹ ApiGateway çš„è·¯ç”±å‡½æ•°ï¼Œç›®å‰ Pluto è¿˜ä¸æ”¯æŒè®¾ç½®å†…å­˜å¤§å°ã€‚å¯¹åº”äº AWS Lambda çš„ Function èµ„æºç±»å‹ï¼Œå¯ä»¥è®¾ç½®å†…å­˜å¤§å°ã€‚
